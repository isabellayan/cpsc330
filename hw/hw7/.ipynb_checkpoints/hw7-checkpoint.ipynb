{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02531391",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac098def",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b7cba",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d9de25",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77686b",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3710e74",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. To submit this assignment, follow the instructions below:\n",
    "\n",
    "- **You may work on this assignment in a group (group size <= 4) and submit your assignment as a group.** \n",
    "- Below are some instructions on working as a group.  \n",
    "    - The maximum group size is 4. \n",
    "    - You can choose your own group members. \n",
    "    - Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "    - Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "    - It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. [Here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members) are some instructions on adding group members in Gradescope.  \n",
    "- Upload the .ipynb file to Gradescope.\n",
    "- **If the .ipynb file is too big or doesn't render on Gradescope for some reason, also upload a pdf or html in addition to the .ipynb.** \n",
    "- Make sure that your plots/output are rendered properly in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b40af",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2f5dd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 18, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8d7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a30193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad9e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7f4d4",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39237a0d",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dec7d",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d6e53",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words in `glove_wiki_vectors` embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad46ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5888626f-b19b-413d-a4af-19f42bfa2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lecture 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab82e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('algorithms', 0.8546786904335022),\n",
       " ('optimization', 0.7233700752258301),\n",
       " ('approximation', 0.6973059773445129),\n",
       " ('iterative', 0.6737284064292908),\n",
       " ('computation', 0.661978542804718),\n",
       " ('compute', 0.6441056728363037),\n",
       " ('equation', 0.6433855295181274),\n",
       " ('method', 0.6391006708145142),\n",
       " ('theorem', 0.6305437088012695),\n",
       " ('iteration', 0.6274698376655579)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f319e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soup', 0.7079030275344849),\n",
       " ('noodles', 0.6946969032287598),\n",
       " ('chicken', 0.6627247333526611),\n",
       " ('ramen', 0.6215193271636963),\n",
       " ('soups', 0.6044745445251465),\n",
       " ('peanut', 0.6007318496704102),\n",
       " ('dish', 0.599707305431366),\n",
       " ('dumpling', 0.5937888622283936),\n",
       " ('pasta', 0.5928375720977783),\n",
       " ('satay', 0.5866751670837402)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"noodle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa87ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coffee', 0.77326899766922),\n",
       " ('beer', 0.6795355081558228),\n",
       " ('drinks', 0.6754350066184998),\n",
       " ('drink', 0.6683974266052246),\n",
       " ('lunch', 0.644118070602417),\n",
       " ('fruit', 0.6430026292800903),\n",
       " ('wine', 0.6251739859580994),\n",
       " ('teas', 0.6107999086380005),\n",
       " ('cocktail', 0.5970641374588013),\n",
       " ('juice', 0.5940718054771423)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"tea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98de5693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bean', 0.6648884415626526),\n",
       " ('chili', 0.6585685610771179),\n",
       " ('ginger', 0.6009073853492737),\n",
       " ('pepper', 0.600674033164978),\n",
       " ('sauce', 0.5930271148681641),\n",
       " ('lemon', 0.5920239090919495),\n",
       " ('chicken', 0.5746395587921143),\n",
       " ('soup', 0.5704665184020996),\n",
       " ('cook', 0.5676928162574768),\n",
       " ('fry', 0.5646450519561768)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"curry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e051b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d53ce4",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8343d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0085a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16bca87a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between coast and shore is 0.700\n",
      "The similarity between clothes and closet is 0.546\n",
      "The similarity between old and new is 0.643\n",
      "The similarity between smart and intelligent is 0.755\n",
      "The similarity between dog and cat is 0.880\n",
      "The similarity between tree and lawyer is 0.077\n"
     ]
    }
   ],
   "source": [
    "# from lecture 18\n",
    "for pair in word_pairs:\n",
    "    print(\n",
    "        \"The similarity between %s and %s is %0.3f\"\n",
    "        % (pair[0], pair[1], glove_wiki_vectors.similarity(pair[0], pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755575d",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f07bdf",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Stereotypes and biases in embeddings\n",
    "rubric={points:6}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings or not by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore what kind of stereotypes and biases are encoded in these embeddings. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)   \n",
    "2. Discuss your observations.\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab62e7",
   "metadata": {},
   "source": [
    "An example of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96458afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be03d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ab6cf",
   "metadata": {},
   "source": [
    "An example of using similarity between words to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ebccab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44723597"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b92841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51745194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"black\", \"rich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca3082",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317ee9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>educator</td>\n",
       "      <td>0.585333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>programmers</td>\n",
       "      <td>0.573053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linguist</td>\n",
       "      <td>0.543201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technician</td>\n",
       "      <td>0.537336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>freelance</td>\n",
       "      <td>0.536212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>animator</td>\n",
       "      <td>0.533937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>translator</td>\n",
       "      <td>0.533419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>software</td>\n",
       "      <td>0.496662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychotherapist</td>\n",
       "      <td>0.495662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>technologist</td>\n",
       "      <td>0.488051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Analogy word     Score\n",
       "0         educator  0.585333\n",
       "1      programmers  0.573053\n",
       "2         linguist  0.543201\n",
       "3       technician  0.537336\n",
       "4        freelance  0.536212\n",
       "5         animator  0.533937\n",
       "6       translator  0.533419\n",
       "7         software  0.496662\n",
       "8  psychotherapist  0.495662\n",
       "9     technologist  0.488051"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"programmer\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0e02de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : work :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>working</td>\n",
       "      <td>0.765436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>works</td>\n",
       "      <td>0.705486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worked</td>\n",
       "      <td>0.695678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she</td>\n",
       "      <td>0.694522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study</td>\n",
       "      <td>0.677781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>her</td>\n",
       "      <td>0.659468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>well</td>\n",
       "      <td>0.650842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>children</td>\n",
       "      <td>0.648094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>care</td>\n",
       "      <td>0.646692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>writing</td>\n",
       "      <td>0.645966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0      working  0.765436\n",
       "1        works  0.705486\n",
       "2       worked  0.695678\n",
       "3          she  0.694522\n",
       "4        study  0.677781\n",
       "5          her  0.659468\n",
       "6         well  0.650842\n",
       "7     children  0.648094\n",
       "8         care  0.646692\n",
       "9      writing  0.645966"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"work\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c52ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23315574"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"men\", \"engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30548ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11648941"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"women\", \"engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b3c598e-932f-4d04-af8d-e4213db9aaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16145678"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"men\", \"scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a565a3b5-e85c-4b1e-9788-020a3c53ddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14743489"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"women\", \"scientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c34406-e7cd-4219-a4f9-a71c4e639b03",
   "metadata": {},
   "source": [
    "From the first analogy, since many people from the stereotype think programming is dominated by men, I want to see if the analogy of man : programmer :: woman : ? will be dominated with occupations with more women, such as a nurse. However, it seems to be fine as it contains all kinds of jobs. The second analogy is deduced from the stereotype that in a family men work but women are homemakers. The words with highest similarity are still related to \"work\", but we can still see words such as \"children\" and \"care\" which suggests women are housewifes. The last two similarities are dealing with how the stereotype considers men as better in science and engineering. From the similarity score it does seem to be showing so, with men having a higher similarity score in comparison to women for the words \"engineer\" and \"scientist\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cff6e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14948ad",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Discussion\n",
    "rubric={points:4}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Based on your exploration above, comment on the overall quality of these pre-trained embeddings. \n",
    "2. In the lecture, we saw that our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30690fa3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13ca08",
   "metadata": {},
   "source": [
    "If we exclude the consideration of stereotype, the quality of the embeddings are relatively well. The similar words given for a single word are all around the same topic, and the cosine similarity of related words does have a higher similarity score than relatively unrelated words. For example, the word \"tea\" is similar to other drinks such as coffee and beer. The similarity between dog and cat is 0.88, since they're both the most common pets. On the other hand, the similarity between tree and lawyer is only 0.077, and we can't find much relationship from our perspective. However, a few stereotypes could still be noticed from the embeddings. \n",
    "\n",
    "Although the embeddings themselves have no cognition and simply output these results from their training data, outputting that women should be homemakers while men work is not a good idea as the society is promoting equality and equity. There will be many people feeling angry about the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7c7b5",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe4f7e",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Classification with pre-trained embeddings \n",
    "rubric={points:8}\n",
    "\n",
    "In lecture 18, we saw that you can conveniently get word vectors with `spaCy` with `en_core_web_md` model. In this exercise, you'll use word embeddings in multi-class text classification task. We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk](https://www.mturk.com/). The ground truth label is not available for all examples, and in this lab, we'll only use the examples where ground truth is available (~15,000 examples). \n",
    "\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it under data/ directory in this homework directory.\n",
    "\n",
    "The code below reads the data CSV (assuming that it's present in the current directory as *cleaned_hm.csv*),  cleans it up a bit, and splits it into train and test splits. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Train logistic regression with bag-of-words features (`CountVectorizer`) and show [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) on the test set. \n",
    "2. Train logistic regression with average embedding representation extracted using spaCy and show [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) on the test set. (You can refer to lecture 18 notes for this.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b519c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>45</td>\n",
       "      <td>24h</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>leisure</td>\n",
       "      <td>leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27697</th>\n",
       "      <td>498</td>\n",
       "      <td>24h</td>\n",
       "      <td>My grandmother start to walk from the bed afte...</td>\n",
       "      <td>My grandmother start to walk from the bed afte...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>affection</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27705</th>\n",
       "      <td>5732</td>\n",
       "      <td>24h</td>\n",
       "      <td>I picked my daughter up from the airport and w...</td>\n",
       "      <td>I picked my daughter up from the airport and w...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>bonding</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27715</th>\n",
       "      <td>2272</td>\n",
       "      <td>24h</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        wid reflection_period  \\\n",
       "hmid                            \n",
       "27676   206               24h   \n",
       "27678    45               24h   \n",
       "27697   498               24h   \n",
       "27705  5732               24h   \n",
       "27715  2272               24h   \n",
       "\n",
       "                                             original_hm  \\\n",
       "hmid                                                       \n",
       "27676  We had a serious talk with some friends of our...   \n",
       "27678                            I meditated last night.   \n",
       "27697  My grandmother start to walk from the bed afte...   \n",
       "27705  I picked my daughter up from the airport and w...   \n",
       "27715        when i received flowers from my best friend   \n",
       "\n",
       "                                              cleaned_hm  modified  \\\n",
       "hmid                                                                 \n",
       "27676  We had a serious talk with some friends of our...      True   \n",
       "27678                            I meditated last night.      True   \n",
       "27697  My grandmother start to walk from the bed afte...      True   \n",
       "27705  I picked my daughter up from the airport and w...      True   \n",
       "27715        when i received flowers from my best friend      True   \n",
       "\n",
       "       num_sentence ground_truth_category predicted_category  \n",
       "hmid                                                          \n",
       "27676             2               bonding            bonding  \n",
       "27678             1               leisure            leisure  \n",
       "27697             1             affection          affection  \n",
       "27705             1               bonding          affection  \n",
       "27715             1               bonding            bonding  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cleaned_hm.csv\", index_col=0)\n",
    "sample_df = df.dropna()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c12baf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXKa7qfQXYPD",
    "outputId": "8bbf5eeb-0151-4853-a49c-3876279bbeb7"
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"target\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69f97ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sample_df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df[\"moment\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87028f3",
   "metadata": {},
   "source": [
    "You need `spacy` to run the code below. If it's not in your course conda environment, you need to install it. \n",
    "\n",
    "> conda install -c conda-forge spacy\n",
    "\n",
    "You also need to download the following language model. \n",
    "\n",
    "> python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22db6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60120920",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4022f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00am</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>07</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoey</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zootopia</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I sorted and organized all my tax documents in preparation to file my taxes.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Last week my dog had 8 puppies, adorable !</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I received my computer certification.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>An event that made me happy was seeing my girlfriend yesterday. We went hiking and got to spend a lot of time together which made me extremely happy. I was happy for 4-6 hours after that.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>My daughter had her 6 year old birthday.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    00  000  0000  00am  04  \\\n",
       "moment                                                                        \n",
       "I sorted and organized all my tax documents in ...   0    0     0     0   0   \n",
       "Last week my dog had 8 puppies, adorable !           0    0     0     0   0   \n",
       "I received my computer certification.                0    0     0     0   0   \n",
       "An event that made me happy was seeing my girlf...   0    0     0     0   0   \n",
       "My daughter had her 6 year old birthday.             0    0     0     0   0   \n",
       "\n",
       "                                                    05  07  10  100  1000  \\\n",
       "moment                                                                      \n",
       "I sorted and organized all my tax documents in ...   0   0   0    0     0   \n",
       "Last week my dog had 8 puppies, adorable !           0   0   0    0     0   \n",
       "I received my computer certification.                0   0   0    0     0   \n",
       "An event that made me happy was seeing my girlf...   0   0   0    0     0   \n",
       "My daughter had her 6 year old birthday.             0   0   0    0     0   \n",
       "\n",
       "                                                    ...  yummy  zealand  \\\n",
       "moment                                              ...                   \n",
       "I sorted and organized all my tax documents in ...  ...      0        0   \n",
       "Last week my dog had 8 puppies, adorable !          ...      0        0   \n",
       "I received my computer certification.               ...      0        0   \n",
       "An event that made me happy was seeing my girlf...  ...      0        0   \n",
       "My daughter had her 6 year old birthday.            ...      0        0   \n",
       "\n",
       "                                                    zelda  zero  zoey  \\\n",
       "moment                                                                  \n",
       "I sorted and organized all my tax documents in ...      0     0     0   \n",
       "Last week my dog had 8 puppies, adorable !              0     0     0   \n",
       "I received my computer certification.                   0     0     0   \n",
       "An event that made me happy was seeing my girlf...      0     0     0   \n",
       "My daughter had her 6 year old birthday.                0     0     0   \n",
       "\n",
       "                                                    zombies  zoo  zootopia  \\\n",
       "moment                                                                       \n",
       "I sorted and organized all my tax documents in ...        0    0         0   \n",
       "Last week my dog had 8 puppies, adorable !                0    0         0   \n",
       "I received my computer certification.                     0    0         0   \n",
       "An event that made me happy was seeing my girlf...        0    0         0   \n",
       "My daughter had her 6 year old birthday.                  0    0         0   \n",
       "\n",
       "                                                    zverev  zz  \n",
       "moment                                                          \n",
       "I sorted and organized all my tax documents in ...       0   0  \n",
       "Last week my dog had 8 puppies, adorable !               0   0  \n",
       "I received my computer certification.                    0   0  \n",
       "An event that made me happy was seeing my girlf...       0   0  \n",
       "My daughter had her 6 year old birthday.                 0   0  \n",
       "\n",
       "[5 rows x 8060 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from lecture 6\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "X_counts = vec.fit_transform(X_train)\n",
    "bow_df = pd.DataFrame(\n",
    "    X_counts.toarray(), columns=vec.get_feature_names_out(), index=X_train\n",
    ")\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de27f837",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer(stop_words='english')),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from lecture 7\n",
    "pipe_lr = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c0bf346-6798-4ea5-94f9-07f90ce961b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['achievement', 'affection', 'bonding', 'enjoy_the_moment',\n",
       "       'exercise', 'leisure', 'nature'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f290eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.79      0.87      0.83      1302\n",
      "       affection       0.90      0.91      0.91      1423\n",
      "         bonding       0.91      0.85      0.88       492\n",
      "enjoy_the_moment       0.60      0.54      0.57       469\n",
      "        exercise       0.91      0.57      0.70        74\n",
      "         leisure       0.73      0.70      0.71       407\n",
      "          nature       0.73      0.46      0.57        71\n",
      "\n",
      "        accuracy                           0.82      4238\n",
      "       macro avg       0.80      0.70      0.74      4238\n",
      "    weighted avg       0.82      0.82      0.81      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, pipe_lr.predict(X_test), target_names=pipe_lr.classes_\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17c55b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lecture 18\n",
    "X_train_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_train)])\n",
    "X_test_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "316152cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.690874</td>\n",
       "      <td>-0.755216</td>\n",
       "      <td>-3.279650</td>\n",
       "      <td>-1.111956</td>\n",
       "      <td>2.333812</td>\n",
       "      <td>1.712701</td>\n",
       "      <td>0.346985</td>\n",
       "      <td>5.052109</td>\n",
       "      <td>-1.078272</td>\n",
       "      <td>1.564646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978959</td>\n",
       "      <td>0.228331</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>-1.340969</td>\n",
       "      <td>-0.884684</td>\n",
       "      <td>1.971317</td>\n",
       "      <td>-0.880458</td>\n",
       "      <td>-0.599687</td>\n",
       "      <td>-5.299727</td>\n",
       "      <td>-0.139515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021659</td>\n",
       "      <td>-1.051312</td>\n",
       "      <td>-4.063361</td>\n",
       "      <td>-1.818072</td>\n",
       "      <td>3.027572</td>\n",
       "      <td>-1.226410</td>\n",
       "      <td>0.035204</td>\n",
       "      <td>2.806751</td>\n",
       "      <td>-0.181556</td>\n",
       "      <td>1.530708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873182</td>\n",
       "      <td>-1.440154</td>\n",
       "      <td>0.219029</td>\n",
       "      <td>-1.426304</td>\n",
       "      <td>-1.591761</td>\n",
       "      <td>-0.475234</td>\n",
       "      <td>-0.122053</td>\n",
       "      <td>0.695197</td>\n",
       "      <td>-4.421231</td>\n",
       "      <td>-0.027261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101639</td>\n",
       "      <td>-1.674547</td>\n",
       "      <td>-1.391433</td>\n",
       "      <td>-3.230565</td>\n",
       "      <td>0.636887</td>\n",
       "      <td>0.756545</td>\n",
       "      <td>0.205278</td>\n",
       "      <td>5.351284</td>\n",
       "      <td>-2.872737</td>\n",
       "      <td>3.033967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594443</td>\n",
       "      <td>0.435623</td>\n",
       "      <td>-0.556652</td>\n",
       "      <td>-2.759545</td>\n",
       "      <td>-1.521208</td>\n",
       "      <td>0.728254</td>\n",
       "      <td>-0.365033</td>\n",
       "      <td>1.230975</td>\n",
       "      <td>-5.989079</td>\n",
       "      <td>0.142662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.502830</td>\n",
       "      <td>-0.245505</td>\n",
       "      <td>-2.919757</td>\n",
       "      <td>-1.312146</td>\n",
       "      <td>3.315014</td>\n",
       "      <td>-0.281209</td>\n",
       "      <td>1.019417</td>\n",
       "      <td>3.909962</td>\n",
       "      <td>-0.440415</td>\n",
       "      <td>0.797455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452126</td>\n",
       "      <td>-0.261931</td>\n",
       "      <td>-0.375499</td>\n",
       "      <td>-0.218783</td>\n",
       "      <td>-1.355882</td>\n",
       "      <td>0.384430</td>\n",
       "      <td>0.619531</td>\n",
       "      <td>0.548577</td>\n",
       "      <td>-3.217188</td>\n",
       "      <td>0.119259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.503838</td>\n",
       "      <td>-0.450076</td>\n",
       "      <td>-4.626111</td>\n",
       "      <td>-2.820389</td>\n",
       "      <td>4.793096</td>\n",
       "      <td>-0.805586</td>\n",
       "      <td>1.518527</td>\n",
       "      <td>5.712772</td>\n",
       "      <td>1.548753</td>\n",
       "      <td>1.589522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293538</td>\n",
       "      <td>-0.217686</td>\n",
       "      <td>-3.700689</td>\n",
       "      <td>-1.188137</td>\n",
       "      <td>0.648456</td>\n",
       "      <td>-2.548641</td>\n",
       "      <td>-0.413536</td>\n",
       "      <td>1.096762</td>\n",
       "      <td>-3.927443</td>\n",
       "      <td>-1.008204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.690874 -0.755216 -3.279650 -1.111956  2.333812  1.712701  0.346985   \n",
       "1  0.021659 -1.051312 -4.063361 -1.818072  3.027572 -1.226410  0.035204   \n",
       "2 -0.101639 -1.674547 -1.391433 -3.230565  0.636887  0.756545  0.205278   \n",
       "3 -1.502830 -0.245505 -2.919757 -1.312146  3.315014 -0.281209  1.019417   \n",
       "4 -0.503838 -0.450076 -4.626111 -2.820389  4.793096 -0.805586  1.518527   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  5.052109 -1.078272  1.564646  ...  0.978959  0.228331  0.843013 -1.340969   \n",
       "1  2.806751 -0.181556  1.530708  ...  0.873182 -1.440154  0.219029 -1.426304   \n",
       "2  5.351284 -2.872737  3.033967  ...  0.594443  0.435623 -0.556652 -2.759545   \n",
       "3  3.909962 -0.440415  0.797455  ...  0.452126 -0.261931 -0.375499 -0.218783   \n",
       "4  5.712772  1.548753  1.589522  ... -0.293538 -0.217686 -3.700689 -1.188137   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.884684  1.971317 -0.880458 -0.599687 -5.299727 -0.139515  \n",
       "1 -1.591761 -0.475234 -0.122053  0.695197 -4.421231 -0.027261  \n",
       "2 -1.521208  0.728254 -0.365033  1.230975 -5.989079  0.142662  \n",
       "3 -1.355882  0.384430  0.619531  0.548577 -3.217188  0.119259  \n",
       "4  0.648456 -2.548641 -0.413536  1.096762 -3.927443 -1.008204  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "138806c9-c8f1-49f6-b327-2cd204d712e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.958416</td>\n",
       "      <td>3.125293</td>\n",
       "      <td>-4.034988</td>\n",
       "      <td>-2.041907</td>\n",
       "      <td>2.595106</td>\n",
       "      <td>0.445679</td>\n",
       "      <td>0.386465</td>\n",
       "      <td>5.083802</td>\n",
       "      <td>-1.852383</td>\n",
       "      <td>1.560549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379911</td>\n",
       "      <td>-0.726037</td>\n",
       "      <td>-1.127668</td>\n",
       "      <td>-0.563859</td>\n",
       "      <td>0.177253</td>\n",
       "      <td>-0.294613</td>\n",
       "      <td>-0.855359</td>\n",
       "      <td>1.358393</td>\n",
       "      <td>-7.270348</td>\n",
       "      <td>1.399086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.299894</td>\n",
       "      <td>0.630366</td>\n",
       "      <td>-2.318005</td>\n",
       "      <td>-2.507104</td>\n",
       "      <td>4.050345</td>\n",
       "      <td>-0.310809</td>\n",
       "      <td>0.272773</td>\n",
       "      <td>3.979269</td>\n",
       "      <td>1.976808</td>\n",
       "      <td>0.731067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931720</td>\n",
       "      <td>0.025334</td>\n",
       "      <td>-0.485929</td>\n",
       "      <td>-0.743087</td>\n",
       "      <td>-1.798334</td>\n",
       "      <td>2.132924</td>\n",
       "      <td>0.299637</td>\n",
       "      <td>0.670521</td>\n",
       "      <td>-3.445617</td>\n",
       "      <td>0.822201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.671838</td>\n",
       "      <td>-3.753560</td>\n",
       "      <td>-2.243932</td>\n",
       "      <td>-6.034901</td>\n",
       "      <td>-2.993384</td>\n",
       "      <td>2.103476</td>\n",
       "      <td>-0.182704</td>\n",
       "      <td>1.314220</td>\n",
       "      <td>0.393286</td>\n",
       "      <td>0.491056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021817</td>\n",
       "      <td>0.214875</td>\n",
       "      <td>-1.920058</td>\n",
       "      <td>-0.585280</td>\n",
       "      <td>-2.042112</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>-0.503640</td>\n",
       "      <td>0.496954</td>\n",
       "      <td>-0.952958</td>\n",
       "      <td>-0.096886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.285316</td>\n",
       "      <td>-1.347252</td>\n",
       "      <td>-5.520160</td>\n",
       "      <td>-1.793326</td>\n",
       "      <td>-0.319224</td>\n",
       "      <td>-4.395940</td>\n",
       "      <td>-0.787140</td>\n",
       "      <td>3.501826</td>\n",
       "      <td>-2.966559</td>\n",
       "      <td>4.460822</td>\n",
       "      <td>...</td>\n",
       "      <td>2.226200</td>\n",
       "      <td>-1.617096</td>\n",
       "      <td>1.458074</td>\n",
       "      <td>-4.914721</td>\n",
       "      <td>-2.545942</td>\n",
       "      <td>-3.723066</td>\n",
       "      <td>-2.130358</td>\n",
       "      <td>1.875884</td>\n",
       "      <td>-8.811860</td>\n",
       "      <td>-0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.296173</td>\n",
       "      <td>1.518396</td>\n",
       "      <td>-3.726718</td>\n",
       "      <td>-1.118125</td>\n",
       "      <td>1.690331</td>\n",
       "      <td>-1.578362</td>\n",
       "      <td>0.601601</td>\n",
       "      <td>5.086967</td>\n",
       "      <td>-0.884238</td>\n",
       "      <td>3.076164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435902</td>\n",
       "      <td>0.211110</td>\n",
       "      <td>1.393579</td>\n",
       "      <td>0.039486</td>\n",
       "      <td>-2.776130</td>\n",
       "      <td>1.652559</td>\n",
       "      <td>3.089273</td>\n",
       "      <td>2.418594</td>\n",
       "      <td>-3.963638</td>\n",
       "      <td>0.478087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.958416  3.125293 -4.034988 -2.041907  2.595106  0.445679  0.386465   \n",
       "1 -1.299894  0.630366 -2.318005 -2.507104  4.050345 -0.310809  0.272773   \n",
       "2  1.671838 -3.753560 -2.243932 -6.034901 -2.993384  2.103476 -0.182704   \n",
       "3  3.285316 -1.347252 -5.520160 -1.793326 -0.319224 -4.395940 -0.787140   \n",
       "4 -3.296173  1.518396 -3.726718 -1.118125  1.690331 -1.578362  0.601601   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  5.083802 -1.852383  1.560549  ... -0.379911 -0.726037 -1.127668 -0.563859   \n",
       "1  3.979269  1.976808  0.731067  ...  0.931720  0.025334 -0.485929 -0.743087   \n",
       "2  1.314220  0.393286  0.491056  ... -0.021817  0.214875 -1.920058 -0.585280   \n",
       "3  3.501826 -2.966559  4.460822  ...  2.226200 -1.617096  1.458074 -4.914721   \n",
       "4  5.086967 -0.884238  3.076164  ...  0.435902  0.211110  1.393579  0.039486   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.177253 -0.294613 -0.855359  1.358393 -7.270348  1.399086  \n",
       "1 -1.798334  2.132924  0.299637  0.670521 -3.445617  0.822201  \n",
       "2 -2.042112  1.066200 -0.503640  0.496954 -0.952958 -0.096886  \n",
       "3 -2.545942 -3.723066 -2.130358  1.875884 -8.811860 -0.704100  \n",
       "4 -2.776130  1.652559  3.089273  2.418594 -3.963638  0.478087  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74fd393e-b736-4bc2-8092-1a04d8478eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LogisticRegression(max_iter=10000)\n",
    "lgr.fit(X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26907082-b2ea-4be2-bf82-6a253d40f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.81      0.83      0.82      1302\n",
      "       affection       0.86      0.91      0.89      1423\n",
      "         bonding       0.83      0.77      0.80       492\n",
      "enjoy_the_moment       0.56      0.51      0.54       469\n",
      "        exercise       0.68      0.76      0.72        74\n",
      "         leisure       0.72      0.65      0.68       407\n",
      "          nature       0.69      0.72      0.70        71\n",
      "\n",
      "        accuracy                           0.79      4238\n",
      "       macro avg       0.74      0.74      0.73      4238\n",
      "    weighted avg       0.79      0.79      0.79      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_test, lgr.predict(X_test_embeddings), target_names=lgr.classes_\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cac00d",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cea614",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.6 Discussion\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Briefly explain the difference between using `CountVectorizer` vs. average-embedding approach for text classification.  \n",
    "2. Which representation among these two would be more interpretable? Why?   \n",
    "3. Are we using any transfer learning here? If yes, are you observing any benefits of transfer learning? Briefly discuss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41f96e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.6\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e29b6",
   "metadata": {},
   "source": [
    "CountVectorizer count the number of occurrence of each word that appeared in the training set for each tuple and stored it in a vector. On the other hand, average-embedding embed each tuple as a whole rather than single words. The test score of the two models are similar where the average-embedded one is slightly lower. However, the macro average recall score for average-embedding is higher, showing it missed less tuples that should be classified in a set giving equal importance to all classes (the range of recall scores is larger in using a CountVectorizer than using average-embedding). \n",
    "\n",
    "CountVectorizer would be more interpretable since we can see what words appeared in each tuple. For average-embedding, we can't interpret anything since we can only see 300 columns with different numbers for each tuple. \n",
    "\n",
    "We are using transfer learning here since the spaCy language model was trained on a completely different corpus. We can observe the benefit of transfer learning here since it could embed all these tuples and result in a test score similar to CountVectorizer while it was trained from a completely different set of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15070c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0fe85",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The overarching goal of topic modeling is understanding high-level themes in a large collection of texts in an unsupervised way. \n",
    "\n",
    "In this exercise you will explore topics in a subset of `scikit-learn`'s [20 newsgroups text dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e429d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4747dc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08b414ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb3884",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc50b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points:8}\n",
    "\n",
    "In this exercise you'll prepare the data for topic modeling using [spaCy](https://spacy.io/). Preprocessing is a crucial step before training an LDA model and it markedly affects topic modeling results. So let's carry out preprocessing. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "Write code to carry out preprocessing of the \"text\" column in the dataframe above and store the preprocessed text in a new column called \"text_pp\" in the dataframe. \n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are more informative and which ones are less informative for the given task. Some common text preprocessing steps for topic modeling are: \n",
    "- getting rid of slashes or other weird characters\n",
    "- sentence segmentation and tokenization      \n",
    "- getting rid of urls and email addresses\n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization        \n",
    "\n",
    "You might have to go back and forth between the preprocessing and topic modeling and interpretation steps in the next exercises. \n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://spacy.io/api/annotation/#pos-en) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you're done with the preprocessing part, you might want to save the preprocessed data so that you don't run the preprocessing part every time you run the notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd206e61",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164705e8",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "871d7c78-7afe-4b2d-8580-b9ee4d500ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kirito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "066a45d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e\n",
    "# from https://datagy.io/python-remove-special-characters-from-string/\n",
    "# from https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "# from https://stackoverflow.com/questions/33113338/how-to-replace-dash-between-characters-with-space-using-regex\n",
    "# from https://www.digitalocean.com/community/tutorials/python-remove-spaces-from-string\n",
    "# from https://datagy.io/python-remove-newline-character-string/\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(s, nlp): \n",
    "    s = re.sub(r'([A-z])\\-([A-z])', r'\\1 \\2', s)\n",
    "    s = re.sub(r'([A-z])\\/([A-z])', r'\\1 \\2', s)\n",
    "    s = re.sub('\\n', ' ', s)\n",
    "    s = s.lower().strip()\n",
    "    \n",
    "    doc = nlp(s)\n",
    "    stop_words = list(set(stopwords.words(\"english\")))\n",
    "    \n",
    "    lemmas = \"\"\n",
    "    for token in doc:\n",
    "        if (token.lemma_ not in stop_words) and (not token.like_email) and (not token.like_url) and (not token.is_punct):\n",
    "            lemmas += \" \"\n",
    "            lemmas += token.lemma_\n",
    "    \n",
    "    lemmas = re.sub(r'\\s+', ' ', lemmas)\n",
    "    return lemmas[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9a27c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text_pp_array = []\n",
    "# for i in range(len(df)):\n",
    "#     preprocessed = preprocess(df.iloc[i]['text'],nlp)\n",
    "#     text_pp_array.append(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1721b793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[\"text_pp\"] = text_pp_array\n",
    "# df.to_csv('data/out.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fcdcbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c4ade12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>know I read 18 u.s.c 922 something make sence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>bad question I ref list algorithm either think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay I hand I support idea one newsgroup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>following problem really bug I I would appreci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>hi everyone I look software call shadow far I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>archive name x faq part3 last modify 1993/04/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>nice answer question difference feds mandate l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>hi I get gateway 4dx-33v try configure x11r5 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>h armenians nagarno karabagh simply defend rig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name                                            text_pp  \n",
       "0        talk.politics.guns  know I read 18 u.s.c 922 something make sence ...  \n",
       "1             comp.graphics  bad question I ref list algorithm either think...  \n",
       "2             comp.graphics  actuallay I hand I support idea one newsgroup ...  \n",
       "3            comp.windows.x  following problem really bug I I would appreci...  \n",
       "4     talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  \n",
       "...                     ...                                                ...  \n",
       "4558          comp.graphics  hi everyone I look software call shadow far I ...  \n",
       "4559         comp.windows.x  archive name x faq part3 last modify 1993/04/0...  \n",
       "4560     talk.politics.guns  nice answer question difference feds mandate l...  \n",
       "4561         comp.windows.x  hi I get gateway 4dx-33v try configure x11r5 a...  \n",
       "4562  talk.politics.mideast  h armenians nagarno karabagh simply defend rig...  \n",
       "\n",
       "[4563 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05fa40",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c2097",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Outline the preprocessing steps you carried out in the previous exercise and provide a brief justification for these steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4648346",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ec3fa",
   "metadata": {},
   "source": [
    "I get rid of new line (\\n) characters with regex, excluded all the punctuations, urls, email addresses, stopwords, and lemmatized each token using spaCy. I get rid of the special characters, punctuations, and stopwords since they generally occur too frequently and doesn't contain enough meaning to classify the text into targets. I removed the urls and email addresses since they're relatively unique and we couldn't classify using these things. I also lemmatized the tokens since for example we want to treat \"walking\", \"walk\", and \"walked\" as a same word so we could better classify the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbca20",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da6589",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points:4}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create a topic model on the preprocessed data using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Pick a reasonable number for `n_components`, i.e., number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b843fb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d3f8d47-2183-49f6-82a3-f9f0828d0eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4455x38421 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 326967 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "X_train_text_cv = count_vec.fit_transform(df[\"text_pp\"].dropna())\n",
    "X_train_text_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48219da9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lecture 18\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=8, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "lda.fit(X_train_text_cv) \n",
    "document_topics = lda.transform(X_train_text_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c41e5-f570-4cb1-9027-bdd6b56c0781",
   "metadata": {},
   "source": [
    "I used 8 topics since this is the number of targets classified by the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ed06e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416aad1",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bbf17",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbdba9e8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29902, 20114,  7182, ..., 31946, 18907, 16150],\n",
       "       [26984, 35196, 32995, ..., 17055, 15184, 33103],\n",
       "       [26984, 35196, 32995, ...,   298, 33103, 15184],\n",
       "       ...,\n",
       "       [ 7182, 29902, 20114, ..., 19213, 24958, 15690],\n",
       "       [29902, 20114,  7182, ..., 20830, 24958, 26667],\n",
       "       [ 7182, 20114, 29902, ..., 24958, 33475, 36797]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from lecture 18\n",
    "np.argsort(lda.components_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03339c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(count_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7224f5cb-2782-4be3-aed0-576a0b7f4219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "gun           team          game          use           armenian      \n",
      "israel        game          team          file          say           \n",
      "state         hockey        10            program       go            \n",
      "use           player        play          image         people        \n",
      "law           nhl           win           window        one           \n",
      "right         league        25            get           turkish       \n",
      "would         play          year          include       come          \n",
      "israeli       gm            11            available     know          \n",
      "firearm       new           12            entry         see           \n",
      "weapon        division      55            server        take          \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       \n",
      "--------      --------      --------      \n",
      "god           point         would         \n",
      "one           one           think         \n",
      "jesus         lib           one           \n",
      "say           problem       say           \n",
      "believe       cub           get           \n",
      "church        time          well          \n",
      "would         edge          people        \n",
      "know          xmu           make          \n",
      "people        like          know          \n",
      "bible         book          like          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(8),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06b9f1-2195-4a94-b698-5ab086edc21d",
   "metadata": {},
   "source": [
    "Topic 5 is likely about Christian since the highest ranked word is god. Topic 3 is likely about computer and graphics as it contains related words. Topic 4 is likely about Mideast since it's related to Armenian and Turkish. Topic 1 is likely on hockey since it contains the word hockey, but it's uncertain whether topic 2 is about baseball since it contains a lot of numbers. Topic 0 is likely on guns as gun is listed as the most frequent word, topic 6 is probably about Windows X since lib is the Windows Library file, and topic 7, although relatively indistinguishable, could be Atheism as it's the only topic left from the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fead4d",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f263c",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c7c4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b9de8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You know, I was reading 18 U.S.C. 922 and something just did not make \\nsence and I was wondering if someone could help me out.\\n\\nSay U.S.C. 922 :\\n\\n(1) Except as provided in paragraph (2), it shall be unlawful for\\nany person to transfer or possess a machinegun.\\n\\n Well I got to looking in my law dictionary and I found that a \"person\" \\nmight also be an artificial entity that is created by government \\nand has no rights under the federal constitution. So, what I \\ndon\\'t understand is how a statute like 922 can be enforced on \\nan individual. So someone tell me how my government can tell\\nme what I can or cannot possess. Just passing a law \\ndoes not make it LAW. Everyone knows that laws are constitional\\nuntil it goes to court. So, has it ever gone to court, not\\njust your run of the mill \"Ok I had it I am guilty, put me in jail\"\\n\\nHas anyone ever claimed that they had a right to possess and was told\\nby the Supreme Court that they didn\\'t have that right?\\n\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1f3a534-73ee-4429-9994-70dac965213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk.politics.guns'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['target_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8210e5b8-7903-4470-8e94-62f7fa02d7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40298688, 0.00160377, 0.00160378, 0.00160408, 0.23583344,\n",
       "       0.00160509, 0.00160496, 0.353158  ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fed66db3-c8c0-4dde-b683-502f4f6fabf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nIt's not a bad question: I don't have any refs that list this algorithm\\neither. But thinking about it a bit, it shouldn't be too hard.\\n\\n1) Take three of the points and find the plane they define as well as\\nthe circle that they lie on (you say you have this algorithm already)\\n\\n2) Find the center  of this circle. The line passing through this center\\nperpendicular to the plane of the three points passes through the center of\\nthe sphere.\\n\\n3) Repeat with the unused point and two of the original points. This\\ngives you two different lines that both pass through the sphere's\\norigin. Their interection is the center of the sphere.\\n\\n4) the radius is easy to compute, it's just the distance from the center to\\nany of the original points.\\n\\nI'll leave the math to you, but this is a workable algorithm. :-)\\n\\n\\nAn alternate method would be to take pairs of points: the plane formed\\nby the perpendicular bisector of each line segment pair also contains the\\ncenter of the sphere. Three pairs will form three planes, intersecting\\nat a point. This might be easier to implement.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93e5b690-bdb5-44ea-b16a-0101cb84943d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]['target_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17b04de4-3698-4ef5-819c-c7b192cc40d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00140632, 0.00140604, 0.00140532, 0.00140675, 0.00140714,\n",
       "       0.00140694, 0.99015365, 0.00140784])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8bfa616-52fe-49d7-a78e-78f714c69054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8410cc5-b85b-4e34-9f12-a872c1808e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['target_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39b11d41-46e9-4783-b638-50cf542d8d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00173809, 0.00173787, 0.00173683, 0.18864425, 0.00173943,\n",
       "       0.00173887, 0.22157086, 0.5810938 ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d102d99-ceba-4445-a59e-9b4306d91da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The following problem is really bugging me,\\nand I would appreciate any help.\\n\\nI create two windows:\\n\\nw1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\\nw2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\\n\\n\\nKeypress events in w2 are discarded, but ButtonPress events fall through\\nto w1, with subwindow set to w2.\\n\\nFYI, I'm using xnews/olvwm.\\n\\nAm I doing something fundamentally wrong here?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b55a57d0-44f5-49d8-8d50-012d83bd7a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.windows.x'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]['target_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47829d9d-c343-4998-a20e-5005c8b2a299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40347403, 0.00304975, 0.00305075, 0.5782104 , 0.00305491,\n",
       "       0.00305335, 0.00305074, 0.00305608])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6c0992e-ba54-4a09-a70e-3815d995055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n  This is the latest from UPI \\n\\n     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\\n     closing its air space to all flights to and from Armenia and would\\n     prevent humanitarian aid from reaching the republic overland across\\n     Turkish territory.\\n\\n  \\n   Historically even the most uncivilized of peoples have exhibited \\n   signs of compassion by allowing humanitarian aid to reach civilian\\n   populations. Even the Nazis did this much.\\n\\n   It seems as though from now on Turkey will publicly pronounce \\n   themselves 'hypocrites' should they choose to continue their\\n   condemnation of the Serbians.\\n\\n\\n\\n--\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3ffaa78-6e93-4d4b-9aa2-65e702c3d58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk.politics.mideast'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4]['target_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb439c73-df5f-4445-9df5-c120b8f8340e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00240982, 0.20327933, 0.06924407, 0.00240613, 0.46425471,\n",
       "       0.00240784, 0.00241147, 0.25358663])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f598e-f2a8-41cf-bb12-f71a452cff85",
   "metadata": {},
   "source": [
    "The first document is classified correctly (mostly about guns), which might due to it containing the word gun. The second one is wrongly classified with the set about Windows X (almost all about Windows X). Maybe because they're all about computers and are relatively similar to each other. The third one is also wrongly classified with the set about Atheism (mostly about Atheism), with a relatively small percentage about Windows X and graphics. Maybe it's because the document is less focused on computers but more on how to do their jobs. The forth document is also wrongly classified into mostly about graphics, which might be due to the words being too unique resulting it being hard to classify. The last document is classified correctly, since it contains the issue on Turkey and Armenia and is common in that topic. \n",
    "\n",
    "Overall, since the model uses CountVectorizer, the documents that contains the more frequent words in a topic is better classified than other documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ad790",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93981a50",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points:6}\n",
    "\n",
    "1. In lecture 19, we talked about multi-class classification. Comment on how each model in the list below might be handling multiclass classification. Check `scikit-learn` documentation for each of these models when you answer this question.  \n",
    "    - Decision Tree\n",
    "    - KNN\n",
    "    - Random Forest    \n",
    "    - Logistic Regression\n",
    "    - SVM RBF\n",
    "2. What is transfer learning in natural language processing or computer vision? Briefly explain.     \n",
    "3. In Lecture 19 we briefly discussed how neural networks are sort of like `Pipeline`s, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5bd53",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db002b01",
   "metadata": {},
   "source": [
    "1. *from https://scikit-learn.org/stable/modules/multiclass.html*\n",
    "    - Decision tree is inherently multi-class since for each node it will select the most important feature to split, which is the feature that have the most obvious difference between each class. \n",
    "    - KNN is also inherently multi-class since it take a vote on its neighbors for the predicted class. \n",
    "    - Random forest is also inherently multi-class since it's just a combination of several decision trees. \n",
    "    - Logistic regression could be either inherently multi-class or one vs. the rest by changing the hyperparameter multi_class. If set to 'ovr', then a binary problem is fit for each label as described in class. From sklearn, \"â€˜multinomialâ€™ is unavailable when solver=â€™liblinearâ€™. â€˜autoâ€™ selects â€˜ovrâ€™ if the data is binary, or if solver=â€™liblinearâ€™, and otherwise selects â€˜multinomialâ€™\". \n",
    "    - SVM RBF is one vs one, meaning it will build a binary model for each pair of classes. \n",
    "2. Transfer learning is how we use a pre-trained model that is trained using other data and use it with our tasks. For example in the exercises above, we used the pre-trained model from spaCy to create an average embedding for the text in HappyDB. \n",
    "3. Since transfer learning allow us to use pre-trained models in other models, a complicated neural network can be build with only a small amount of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4124f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bed91",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** \n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from \"1\" will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. \n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb so that the TAs can view your submission on Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bdf56",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
